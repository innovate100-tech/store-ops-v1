# 프로덕션 안정화 진단 리포트
**생성일**: 2026-01-20  
**분석 범위**: 전체 코드베이스  
**목표**: 운영용 앱 수준의 안정성 확보

---

## 🔴 Critical - 즉시 수정 필요 (운영 중단 위험)

### 1. IndexError/KeyError 위험 - 빈 DataFrame 접근
**심각도**: 🔴🔴🔴  
**위치**: `app.py` 전역 (32개 `.iloc[0]` 사용)  
**문제점**:
- `df.iloc[0]` 호출 전 빈 DataFrame 체크 누락 다수
- 예: `menu_info.iloc[0]`, `ingredient_info.iloc[0]`, `target_row.iloc[0]`
- 빈 데이터 시 앱 크래시

**영향**:
- 사용자가 데이터 입력 전 페이지 접근 시 즉시 에러
- 필터링 결과가 없을 때 크래시

**즉시 수정 필요 위치**:
```python
# 위험한 패턴 (32곳 발견)
menu_info = menu_df[menu_df['메뉴명'] == selected_menu].iloc[0]  # 빈 경우 IndexError
target_sales = float(target_row.iloc[0].get('목표매출', 0))  # 빈 경우 IndexError
```

**해결 방안**:
- 모든 `.iloc[0]` 전에 `.empty` 체크 추가
- `.get()` 메서드로 안전하게 접근
- 기본값 제공

---

### 2. 트랜잭션 처리 부족 - 데이터 무결성 위험
**심각도**: 🔴🔴🔴  
**위치**: `src/storage_supabase.py` - 모든 save 함수들  
**문제점**:
- 여러 테이블에 저장하는 작업이 트랜잭션 없이 실행
- 부분 실패 시 데이터 불일치 발생
- 예: `save_daily_close` - sales, visitors, daily_sales_items 등 여러 테이블 저장

**영향**:
- 일부만 저장되고 일부는 실패 → 데이터 불일치
- 복구 불가능한 상태 발생 가능

**즉시 수정 필요 함수**:
- `save_daily_close()` - 여러 테이블 동시 저장
- `save_abc_history()` - 삭제 후 삽입 (원자성 필요)
- `save_order()` - orders + inventory 업데이트

**해결 방안**:
- Supabase RPC 함수로 트랜잭션 래핑
- 또는 저장 순서 보장 + 실패 시 롤백 로직

---

### 3. 예외 처리 미흡 - 사용자 경험 저하
**심각도**: 🔴🔴  
**위치**: `app.py` 전역 (136개 try/except)  
**문제점**:
- `except Exception: pass` 패턴 다수 (20개 이상)
- 에러를 삼켜버려 사용자가 문제를 인지 못함
- `load_csv` 실패 시 빈 DataFrame 반환만 함

**영향**:
- 데이터 로드 실패 시 "데이터 없음"으로 오인
- 디버깅 어려움
- 사용자 불만 증가

**즉시 수정 필요 위치**:
```python
# 위험한 패턴
try:
    load_csv.clear()
except Exception:
    pass  # 에러를 무시함

# load_csv에서
except Exception as e:
    logger.error(...)
    return pd.DataFrame()  # 사용자는 데이터가 없다고 착각
```

**해결 방안**:
- 모든 예외를 로깅
- 사용자에게 명확한 에러 메시지 표시
- 빈 DataFrame 반환 대신 에러 상태 명시

---

### 4. 리소스 누수 위험 - "Too many open files"
**심각도**: 🔴🔴  
**위치**: `app.py` - 캐시 클리어 로직 (50개 `load_csv.clear()` 호출)  
**문제점**:
- `load_csv.clear()` 호출이 과도함
- Supabase 클라이언트 연결이 제대로 닫히지 않을 수 있음
- 캐시 클리어 시점이 불일치

**영향**:
- 이미 발생한 "Too many open files" 에러 재발 가능
- 장시간 사용 시 메모리 누수

**즉시 수정 필요 위치**:
- 모든 `load_csv.clear()` 호출 지점 검토
- 선택적 캐시 무효화로 변경

---

### 5. 동시성 문제 - Race Condition
**심각도**: 🔴🔴  
**위치**: 모든 save 함수들  
**문제점**:
- Optimistic Locking 없음
- 여러 사용자가 동시에 같은 데이터 수정 시 마지막 쓰기만 유지
- Version 필드나 updated_at 체크 없음

**영향**:
- 사용자 A가 수정한 내용이 사용자 B에 의해 덮어쓰기됨
- 데이터 손실

**해결 방안**:
- `updated_at` 타임스탬프 기반 충돌 감지
- 수정 전 데이터 버전 체크
- 충돌 시 사용자에게 알림

---

## 🟡 High Priority - 1주일 내 수정 (성능/안정성 저하)

### 6. 빈 DataFrame 체크 불일치
**심각도**: 🟡🟡  
**위치**: `app.py` 전역 (148개 `.empty` 체크)  
**문제점**:
- 일부는 `if not df.empty:`, 일부는 `if df.empty:` 사용
- 일관성 없는 체크 패턴
- 일부는 체크 없이 바로 접근

**영향**:
- 코드 가독성 저하
- 버그 발생 가능성 증가

**해결 방안**:
- 표준화된 빈 DataFrame 체크 패턴 적용
- 헬퍼 함수 생성: `safe_get_first_row(df, default=None)`

---

### 7. 과도한 데이터 로딩 - 성능 병목
**심각도**: 🟡🟡  
**위치**: `app.py` - 발주관리 페이지 외 다수  
**문제점**:
- 페이지 로드 시 모든 데이터를 한 번에 로드
- 사용하지 않는 탭의 데이터도 로드
- 중복 로딩 (발주관리 페이지에서 일부 개선했으나 다른 페이지도 동일 문제)

**영향**:
- 초기 로딩 시간 증가
- 메모리 사용량 증가
- 사용자 경험 저하

**해결 방안**:
- 지연 로딩 (Lazy Loading) 적용
- 탭 진입 시에만 데이터 로드
- 필요한 컬럼만 선택적으로 로드

---

### 8. pandas apply 남용 - 성능 저하
**심각도**: 🟡  
**위치**: `app.py` 전역 (다수)  
**문제점**:
- 벡터화 가능한 연산을 `apply`로 처리
- 행별 반복 처리로 느림

**영향**:
- 대용량 데이터 처리 시 매우 느림
- CPU 사용량 증가

**해결 방안**:
- 벡터화 연산으로 전환 (발주관리 페이지에서 일부 개선)
- `apply` 사용 최소화

---

### 9. 캐시 전략 불일치
**심각도**: 🟡  
**위치**: `src/storage_supabase.py:59` - `@st.cache_data(ttl=60)`  
**문제점**:
- 모든 데이터에 동일한 TTL(60초) 적용
- 마스터 데이터(메뉴, 재료)와 트랜잭션 데이터(매출)를 동일하게 처리
- 캐시 무효화 타이밍 불일치

**영향**:
- 마스터 데이터는 너무 자주 갱신
- 트랜잭션 데이터는 너무 늦게 갱신
- 데이터 일관성 문제

**해결 방안**:
- 데이터 타입별 TTL 분리
- 마스터 데이터: TTL=3600 (1시간)
- 트랜잭션 데이터: TTL=60 (1분)
- 또는 동적 TTL 함수

---

### 10. 에러 메시지 일관성 부족
**심각도**: 🟡  
**위치**: 전체 코드베이스  
**문제점**:
- 에러 메시지 형식이 일관되지 않음
- 한국어/영어 혼용
- 기술적 용어 사용

**영향**:
- 사용자 혼란
- 지원 어려움

**해결 방안**:
- 표준화된 에러 메시지 포맷
- 사용자 친화적 메시지
- 에러 코드 체계 도입

---

## 🟢 Medium Priority - 1개월 내 개선 (유지보수성)

### 11. 코드 중복
**심각도**: 🟢  
**위치**: 전체 코드베이스  
**문제점**:
- 유사한 로직이 여러 곳에 반복
- DataFrame 처리 패턴 중복

**해결 방안**:
- 공통 함수 추출
- 유틸리티 모듈 강화

---

### 12. 로깅 시스템 부족
**심각도**: 🟢  
**위치**: 전체 코드베이스  
**문제점**:
- 에러 로깅은 있으나 비즈니스 로직 로깅 부족
- 구조화된 로깅 없음
- 디버깅 정보 부족

**해결 방안**:
- 구조화된 로깅 (JSON 형식)
- 로그 레벨 체계화
- 비즈니스 이벤트 로깅

---

### 13. 테스트 코드 부재
**심각도**: 🟢  
**위치**: 전체 코드베이스  
**문제점**:
- 단위 테스트 없음
- 통합 테스트 없음
- 리팩토링 시 회귀 테스트 불가

**해결 방안**:
- 핵심 함수 단위 테스트 추가
- 통합 테스트 프레임워크 구축

---

## 📊 통계 요약

### 코드 메트릭
- **총 라인 수**: ~8,000+ 라인
- **try/except 블록**: 136개
- **`.iloc[0]` 사용**: 32개 (위험)
- **`.empty` 체크**: 148개
- **`pd.merge` 사용**: 17개
- **`load_csv.clear()` 호출**: 50개
- **`st.rerun()` 호출**: 50개 (일부 제거됨)

### 위험도 분포
- 🔴 Critical: 5개 항목
- 🟡 High Priority: 5개 항목
- 🟢 Medium Priority: 3개 항목

---

## 🎯 즉시 수정 우선순위

### Phase 1 (1-2일 내)
1. ✅ **IndexError 방지** - 모든 `.iloc[0]` 전 빈 DataFrame 체크
2. ✅ **예외 처리 개선** - `except: pass` 제거, 로깅 추가
3. ✅ **빈 DataFrame 안전 접근** - 헬퍼 함수 도입

### Phase 2 (3-5일 내)
4. ✅ **트랜잭션 처리** - 다중 테이블 저장 함수들 보호
5. ✅ **리소스 관리** - 캐시 클리어 최적화
6. ✅ **동시성 보호** - Optimistic Locking 도입

### Phase 3 (1-2주 내)
7. ✅ **성능 최적화** - 지연 로딩, 벡터화 연산
8. ✅ **캐시 전략 개선** - 데이터 타입별 TTL
9. ✅ **에러 메시지 표준화**

---

## 📝 다음 단계

1. **Phase 1부터 시작** - 가장 위험한 부분부터 수정
2. **단계별 테스트** - 각 Phase 완료 후 테스트
3. **모니터링** - 프로덕션 배포 후 에러 모니터링
4. **문서화** - 수정 사항 문서화

---

**리포트 작성자**: 프로덕션 안정화 전담 시니어 엔지니어  
**다음 리뷰**: Phase 1 완료 후
